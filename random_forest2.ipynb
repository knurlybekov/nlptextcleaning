{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T23:44:02.129521Z",
     "start_time": "2025-02-25T23:44:02.008457Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('rest_data.csv')\n",
    "df.head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  category                                               text\n",
       "0     arts  you are using an older browser version. please...\n",
       "1     arts  on 31 march two of classical music s most acco...\n",
       "2     arts  bpt after a year of being locked away at home ...\n",
       "3     arts  pilot uninjured plane hit sandbar while landin...\n",
       "4     arts  colleen distin photo by facebook toronto sun ...."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts</td>\n",
       "      <td>you are using an older browser version. please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts</td>\n",
       "      <td>on 31 march two of classical music s most acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts</td>\n",
       "      <td>bpt after a year of being locked away at home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts</td>\n",
       "      <td>pilot uninjured plane hit sandbar while landin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts</td>\n",
       "      <td>colleen distin photo by facebook toronto sun ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:44:22.624635Z",
     "start_time": "2025-02-25T23:44:15.046189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "import re\n",
    "# Download NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Step 2: Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Step 3: Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Step 4: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 5: Tokenize text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Step 6: Remove stopwords and non-English words\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) >=3]\n",
    "    \n",
    "    # Step 7: Lemmatize words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Step 8: Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv(\"rest_data.csv\")\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "df.head()\n",
    "    \n",
    "\n",
    "    "
   ],
   "id": "1ab206144d9877c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/karennurlybekov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/karennurlybekov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/karennurlybekov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  category                                               text  \\\n",
       "0     arts  you are using an older browser version. please...   \n",
       "1     arts  on 31 march two of classical music s most acco...   \n",
       "2     arts  bpt after a year of being locked away at home ...   \n",
       "3     arts  pilot uninjured plane hit sandbar while landin...   \n",
       "4     arts  colleen distin photo by facebook toronto sun ....   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  using older browser version please use support...  \n",
       "1  march two classical music accomplished well kn...  \n",
       "2  bpt year locked away home world eager reopen e...  \n",
       "3  pilot uninjured plane hit sandbar landing floa...  \n",
       "4  colleen distin photo facebook toronto sun lost...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts</td>\n",
       "      <td>you are using an older browser version. please...</td>\n",
       "      <td>using older browser version please use support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts</td>\n",
       "      <td>on 31 march two of classical music s most acco...</td>\n",
       "      <td>march two classical music accomplished well kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts</td>\n",
       "      <td>bpt after a year of being locked away at home ...</td>\n",
       "      <td>bpt year locked away home world eager reopen e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts</td>\n",
       "      <td>pilot uninjured plane hit sandbar while landin...</td>\n",
       "      <td>pilot uninjured plane hit sandbar landing floa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts</td>\n",
       "      <td>colleen distin photo by facebook toronto sun ....</td>\n",
       "      <td>colleen distin photo facebook toronto sun lost...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:45:28.954059Z",
     "start_time": "2025-02-25T23:44:26.778398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load SpaCy model (disable unused components for speed)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "pos_tags = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON']\n",
    "\n",
    "# Extract POS features\n",
    "features = []\n",
    "for text in df[\"cleaned_text\"]:\n",
    "    doc = nlp(text)\n",
    "    total_tokens = len(doc)\n",
    "    pos_counts = {tag: 0 for tag in pos_tags}\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        if pos in pos_counts:\n",
    "            pos_counts[pos] += 1\n",
    "    if total_tokens > 0:\n",
    "        for tag in pos_counts:\n",
    "            pos_counts[tag] /= total_tokens\n",
    "    features.append(pos_counts)"
   ],
   "id": "50ccd3fa82f2b62f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T23:53:32.200723Z",
     "start_time": "2025-02-25T23:52:17.612987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from numpy import hstack\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label_encoded\"] = label_encoder.fit_transform(df[\"category\"])\n",
    "df = df.dropna(subset=['label_encoded'])\n",
    "y = df[\"label_encoded\"]\n",
    "\n",
    "\n",
    "# Extract POS features (already a DataFrame)\n",
    "X_pos = pd.DataFrame(features)\n",
    "X_pos_sparse = csr_matrix(X_pos.values)  # Convert to sparse matrix\n",
    "\n",
    "# TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_tfidf = tfidf.fit_transform(df[\"cleaned_text\"])\n",
    "\n",
    "# # Word embeddings (convert to sparse)\n",
    "# nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"ner\"])\n",
    "# X_emb = np.array([nlp(text).vector for text in df[\"cleaned_text\"]])\n",
    "# X_emb_sparse = csr_matrix(X_emb)  # Convert to sparse\n",
    "\n",
    "# # Combine all features\n",
    "# X_combined = hstack([X_tfidf, X_pos_sparse,]) #X_emb_sparse\n",
    "\n",
    "import scipy\n",
    "X_combined = scipy.sparse.hstack([X_pos, X_tfidf])\n"
   ],
   "id": "8c84baadcf02738a",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-26T00:35:39.670985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.utils import parallel_backend\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class tqdm_joblib:\n",
    "    def __init__(self, total=None):\n",
    "        self.pbar = None\n",
    "        self.total = total\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.pbar = tqdm(total=self.total)\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.pbar.close()\n",
    "        \n",
    "    def update(self, _):\n",
    "        self.pbar.update(1)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Modified Grid Search\n",
    "# ---------------------------------------------------\n",
    "# Load your data here\n",
    "# X_combined = ...\n",
    "# y = ...\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"class_weight\": [\"balanced\", None]\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "n_combinations = len(ParameterGrid(param_grid))\n",
    "total_fits = n_combinations * 5  # cv=5\n",
    "\n",
    "# Run grid search with progress bar\n",
    "with tqdm_joblib(total=total_fits) as progress:\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Evaluation with Threshold Filtering\n",
    "# ---------------------------------------------------\n",
    "best_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predict probabilities\n",
    "probs = best_clf.predict_proba(X_test)\n",
    "max_probs = np.max(probs, axis=1)\n",
    "pred_labels = best_clf.classes_[np.argmax(probs, axis=1)]\n",
    "\n",
    "# Set a confidence threshold\n",
    "confidence_threshold = 0.5  # Adjust as needed\n",
    "\n",
    "# Replace low-confidence predictions with \"Unknown\"\n",
    "final_preds = [\n",
    "    \"Unknown\" if max_probs[i] < confidence_threshold else pred_labels[i]\n",
    "    for i in range(len(pred_labels))\n",
    "]\n",
    "\n",
    "# Convert y_test to a NumPy array to avoid index issues\n",
    "y_test = y_test.values\n",
    "\n",
    "# Evaluate ignoring \"Unknown\" predictions\n",
    "valid_idx = [i for i in range(len(final_preds)) if final_preds[i] != \"Unknown\"]\n",
    "filtered_y_test = y_test[valid_idx]  # Direct slicing\n",
    "filtered_preds = np.array(final_preds)[valid_idx]\n",
    "\n",
    "# Print overall accuracy and filtered accuracy\n",
    "print(f\"Original Accuracy: {accuracy_score(y_test, pred_labels):.2f}\")\n",
    "if valid_idx:\n",
    "    print(f\"Filtered Accuracy (excluding 'Unknown' cases): {accuracy_score(filtered_y_test, filtered_preds):.2f}\")\n",
    "else:\n",
    "    print(\"All predictions were below the threshold, no valid predictions for evaluation.\")"
   ],
   "id": "1d893eb06bc8cfba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T21:07:22.932941Z",
     "start_time": "2025-02-25T21:07:22.927094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy = DummyClassifier(strategy=\"stratified\").fit(X_train, y_train)\n",
    "print(\"Baseline accuracy:\", dummy.score(X_test, y_test))"
   ],
   "id": "1b810668b540a0c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.04627006610009443\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T00:05:52.071846Z",
     "start_time": "2025-02-26T00:05:51.984729Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2fd6077399e3ae47",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y must have at least two dimensions for multi-output regression but has only one.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[75], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Usage example:\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;66;03m# Assume X_combined and multi-label y (with multiple columns) are already defined.\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     model, X_test, y_test \u001B[38;5;241m=\u001B[39m train_model_multi_label(X_combined, y)\n\u001B[1;32m     28\u001B[0m     evaluate_model_multi_label(model, X_test, y_test)\n",
      "Cell \u001B[0;32mIn[75], line 13\u001B[0m, in \u001B[0;36mtrain_model_multi_label\u001B[0;34m(X, y)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Wrap RandomForestClassifier so that one classifier is fit per label.\u001B[39;00m\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m MultiOutputClassifier(RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m))\n\u001B[0;32m---> 13\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model, X_test, y_test\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/multioutput.py:543\u001B[0m, in \u001B[0;36mMultiOutputClassifier.fit\u001B[0;34m(self, X, Y, sample_weight, **fit_params)\u001B[0m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, Y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params):\n\u001B[1;32m    518\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001B[39;00m\n\u001B[1;32m    519\u001B[0m \n\u001B[1;32m    520\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;124;03m        Returns a fitted instance.\u001B[39;00m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 543\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(X, Y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[1;32m    544\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m=\u001B[39m [estimator\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;28;01mfor\u001B[39;00m estimator \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_]\n\u001B[1;32m    545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/multioutput.py:248\u001B[0m, in \u001B[0;36m_MultiOutputEstimator.fit\u001B[0;34m(self, X, y, sample_weight, **fit_params)\u001B[0m\n\u001B[1;32m    245\u001B[0m     check_classification_targets(y)\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    249\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my must have at least two dimensions for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulti-output regression but has only one.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    251\u001B[0m     )\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _routing_enabled():\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: y must have at least two dimensions for multi-output regression but has only one."
     ]
    }
   ],
   "execution_count": 75
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
