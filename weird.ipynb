{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-01T02:40:50.538514Z",
     "start_time": "2025-03-01T02:40:50.273536Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_1.csv')\n",
    "\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  category                                               text  \\\n",
       "0     arts  rob delaney vir das galen hopper samson kayo g...   \n",
       "1     arts  andris nelsons conducts a joint concert of the...   \n",
       "2     arts  warner music group has brought on sherry tan t...   \n",
       "3     arts  adele will explore what she s been going throu...   \n",
       "4     arts  you are using an older browser version please ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  rob da galen hopper samson kayo guz khan nick ...  \n",
       "1  joint concert boston symphony orchestra visiti...  \n",
       "2  warner music group brought sherry tan head mus...  \n",
       "3  explore going new album set explore going new ...  \n",
       "4  older browser version please use version best ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts</td>\n",
       "      <td>rob delaney vir das galen hopper samson kayo g...</td>\n",
       "      <td>rob da galen hopper samson kayo guz khan nick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts</td>\n",
       "      <td>andris nelsons conducts a joint concert of the...</td>\n",
       "      <td>joint concert boston symphony orchestra visiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts</td>\n",
       "      <td>warner music group has brought on sherry tan t...</td>\n",
       "      <td>warner music group brought sherry tan head mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts</td>\n",
       "      <td>adele will explore what she s been going throu...</td>\n",
       "      <td>explore going new album set explore going new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts</td>\n",
       "      <td>you are using an older browser version please ...</td>\n",
       "      <td>older browser version please use version best ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T02:25:30.898041Z",
     "start_time": "2025-03-01T02:25:30.878382Z"
    }
   },
   "cell_type": "code",
   "source": "df[\"category\"].unique()",
   "id": "925c8193a3aa40a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['arts', 'crime', 'disaster', 'economy', 'education',\n",
       "       'environmental', 'health', 'humanInterest', 'labour', 'lifestyle',\n",
       "       'other', 'politics', 'religion', 'science', 'social', 'sport',\n",
       "       'unrest', 'weather'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T02:42:08.003863Z",
     "start_time": "2025-03-01T02:42:07.675374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def count_common_words(df):\n",
    "    category_word_counts = {}\n",
    "\n",
    "    for category in df[\"category\"].unique():\n",
    "        # Get all text for this category\n",
    "        category_text = \" \".join(df[df[\"category\"] == category][\"text\"])\n",
    "\n",
    "        # Tokenize and count word frequencies\n",
    "        words = category_text.split()\n",
    "        word_counts = Counter(words)\n",
    "\n",
    "        # Store the word frequency dictionary\n",
    "        category_word_counts[category] = dict(word_counts)\n",
    "\n",
    "    return category_word_counts\n",
    "\n",
    "# Get word counts per category\n",
    "word_counts_by_category = count_common_words(df)\n",
    "\n",
    "# Display the top 10 most common words for each category\n",
    "for category, word_counts in word_counts_by_category.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    print(dict(Counter(word_counts).most_common(25)))  # Show top 10 words\n",
    "    print(\"-\" * 40)"
   ],
   "id": "db88750404250060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: arts\n",
      "{'the': 8198, 'and': 4248, 'to': 3802, 'of': 3665, 'a': 3466, 'in': 2918, 's': 1772, 'for': 1676, 'on': 1417, 'is': 1367, 'with': 1277, 'that': 1239, 'it': 1065, 'at': 963, 'as': 853, 'i': 845, 'this': 775, 'was': 754, 'you': 738, 'be': 701, 'from': 681, 'by': 678, 'are': 676, 'will': 665, 'we': 652}\n",
      "----------------------------------------\n",
      "Category: crime\n",
      "{'the': 7911, 'to': 3867, 'a': 3668, 'of': 3541, 'and': 3415, 'in': 3162, 'that': 1466, 's': 1420, 'on': 1329, 'for': 1288, 'was': 1172, 'is': 989, 'with': 967, 'said': 935, 'by': 836, 'he': 826, 'at': 803, 'as': 737, 'it': 686, 'his': 676, 'from': 639, 'police': 608, 'an': 576, 'has': 529, 'be': 506}\n",
      "----------------------------------------\n",
      "Category: disaster\n",
      "{'the': 7803, 'to': 3562, 'and': 3540, 'of': 3281, 'a': 3163, 'in': 2834, 'on': 1324, 'for': 1155, 's': 1136, 'that': 1134, 'is': 1063, 'was': 909, 'said': 894, 'with': 840, 'at': 825, 'from': 734, 'it': 734, 'as': 665, 'by': 607, 'are': 605, 'fire': 579, 'be': 547, 'an': 490, 'have': 483, 'he': 464}\n",
      "----------------------------------------\n",
      "Category: economy\n",
      "{'the': 7250, 'to': 3560, 'and': 3473, 'of': 3400, 'in': 2656, 'a': 2328, 'for': 1459, 'on': 1188, 's': 1139, 'that': 1057, 'is': 1043, 'with': 812, 'as': 789, 'at': 762, 'it': 715, 'by': 677, 'from': 669, 'are': 666, 'be': 610, 'this': 586, 'you': 560, 'will': 524, 'or': 510, 'said': 489, 'an': 470}\n",
      "----------------------------------------\n",
      "Category: education\n",
      "{'the': 8429, 'and': 4537, 'to': 4477, 'of': 4221, 'in': 3356, 'a': 3063, 'for': 1873, 'that': 1338, 'is': 1306, 's': 1250, 'students': 1107, 'on': 1091, 'school': 1031, 'with': 1029, 'at': 979, 'said': 928, 'as': 885, 'be': 753, 'will': 747, 'it': 733, 'from': 727, 'are': 706, 'university': 690, 'by': 662, 'was': 660}\n",
      "----------------------------------------\n",
      "Category: environmental\n",
      "{'the': 8747, 'and': 4696, 'to': 4547, 'of': 4147, 'in': 3155, 'a': 2858, 'for': 1682, 'is': 1483, 'that': 1384, 's': 1295, 'on': 1199, 'with': 1012, 'at': 938, 'said': 932, 'it': 884, 'are': 877, 'as': 826, 'from': 826, 'be': 810, 'by': 775, 'will': 721, 'energy': 662, 'this': 642, 'we': 631, 'have': 603}\n",
      "----------------------------------------\n",
      "Category: health\n",
      "{'the': 8015, 'and': 4645, 'to': 4480, 'of': 4166, 'in': 3189, 'a': 3068, 'for': 1617, 'is': 1463, 'that': 1440, 'with': 1175, 's': 1163, 'on': 1131, 'it': 927, 'as': 906, 'health': 896, 'are': 890, 'at': 858, 'have': 777, 'said': 771, 'by': 708, 'be': 660, 'this': 640, 'from': 639, 'has': 631, 'or': 561}\n",
      "----------------------------------------\n",
      "Category: humaninterest\n",
      "{'the': 7447, 'and': 3745, 'to': 3560, 'of': 3182, 'a': 2960, 'in': 2769, 'for': 1459, 's': 1433, 'on': 1142, 'is': 1048, 'at': 1015, 'with': 996, 'that': 960, 'it': 903, 'as': 700, 'was': 675, 'be': 660, 'are': 635, 'i': 627, 'this': 611, 'you': 575, 'by': 563, 'we': 549, 'have': 547, 'from': 536}\n",
      "----------------------------------------\n",
      "Category: labour\n",
      "{'the': 8097, 'to': 5337, 'and': 4686, 'of': 3843, 'a': 3223, 'in': 3145, 'for': 1836, 'that': 1718, 'on': 1408, 'is': 1383, 's': 1315, 'you': 1153, 'with': 1143, 'it': 1121, 'are': 1061, 'as': 1060, 'at': 927, 'be': 904, 'said': 846, 'have': 810, 'work': 784, 'will': 757, 'this': 740, 'from': 731, 'their': 680}\n",
      "----------------------------------------\n",
      "Category: lifestyle\n",
      "{'the': 7295, 'and': 4472, 'to': 4384, 'a': 3476, 'of': 3316, 'in': 2716, 'for': 1687, 'is': 1402, 's': 1383, 'with': 1383, 'that': 1323, 'on': 1245, 'it': 1184, 'you': 894, 'are': 893, 'as': 864, 'at': 824, 'i': 798, 'this': 754, 'be': 728, 'from': 723, 'we': 655, 'or': 651, 'have': 609, 'by': 607}\n",
      "----------------------------------------\n",
      "Category: other\n",
      "{'the': 5706, 'to': 2726, 'and': 2649, 'of': 2482, 'in': 2205, 'a': 2200, 'for': 1143, 'on': 1030, 's': 1006, 'that': 870, 'is': 828, 'with': 781, 'at': 614, 'as': 590, 'it': 560, 'said': 545, 'by': 478, 'be': 463, 'have': 455, 'from': 454, 'was': 446, 'this': 436, 'he': 421, 'will': 419, 'has': 412}\n",
      "----------------------------------------\n",
      "Category: politics\n",
      "{'the': 10182, 'to': 5144, 'and': 4699, 'of': 4642, 'a': 3599, 'in': 3526, 'for': 1876, 'that': 1842, 'on': 1663, 's': 1610, 'is': 1450, 'with': 1222, 'as': 1019, 'it': 1005, 'said': 957, 'by': 891, 'at': 856, 'was': 842, 'are': 773, 'from': 749, 'have': 733, 'be': 725, 'has': 685, 'this': 672, 'he': 646}\n",
      "----------------------------------------\n",
      "Category: religion\n",
      "{'the': 10806, 'and': 5478, 'of': 5378, 'to': 5139, 'a': 4272, 'in': 3921, 's': 2017, 'that': 1940, 'is': 1910, 'for': 1893, 'with': 1490, 'on': 1480, 'it': 1315, 'as': 1206, 'was': 1084, 'at': 1036, 'he': 961, 'said': 906, 'this': 876, 'are': 875, 'by': 872, 'be': 862, 'from': 856, 'i': 811, 'his': 775}\n",
      "----------------------------------------\n",
      "Category: science\n",
      "{'the': 9179, 'and': 5357, 'of': 4601, 'to': 4463, 'a': 3462, 'in': 3186, 'for': 1821, 'is': 1729, 's': 1496, 'with': 1465, 'that': 1464, 'on': 1269, 'as': 1074, 'it': 1048, 'at': 872, 'are': 867, 'by': 782, 'from': 760, 'this': 735, 'be': 730, 'market': 725, 'will': 674, 'we': 614, 'has': 608, 'an': 595}\n",
      "----------------------------------------\n",
      "Category: social\n",
      "{'the': 8088, 'to': 4741, 'and': 4138, 'of': 3904, 'a': 3261, 'in': 3173, 'that': 1781, 'for': 1639, 'is': 1421, 's': 1387, 'on': 1328, 'with': 1122, 'it': 1027, 'as': 985, 'are': 866, 'at': 816, 'said': 790, 'be': 753, 'was': 743, 'have': 737, 'by': 709, 'this': 692, 'from': 667, 'their': 636, 'we': 636}\n",
      "----------------------------------------\n",
      "Category: sport\n",
      "{'the': 8261, 'and': 3509, 'to': 3508, 'in': 3352, 'a': 3332, 'of': 2706, 's': 1632, 'for': 1516, 'on': 1478, 'with': 1161, 'at': 1021, 'is': 1019, 'it': 961, 'that': 952, 'was': 889, 'as': 799, 'i': 770, 'he': 749, 'be': 657, 'his': 653, 'will': 636, 'we': 622, 'from': 602, 'said': 593, 'this': 576}\n",
      "----------------------------------------\n",
      "Category: unrest\n",
      "{'the': 9338, 'of': 4169, 'and': 4029, 'to': 3942, 'in': 3854, 'a': 3521, 'on': 1561, 's': 1548, 'that': 1443, 'for': 1358, 'is': 1199, 'with': 1109, 'said': 998, 'was': 973, 'as': 875, 'it': 824, 'at': 805, 'from': 755, 'he': 727, 'by': 716, 'are': 624, 'have': 607, 'has': 585, 'we': 580, 'were': 565}\n",
      "----------------------------------------\n",
      "Category: weather\n",
      "{'the': 9413, 'to': 4994, 'and': 4735, 'of': 4505, 'in': 3538, 'a': 3336, 'for': 1677, 'is': 1578, 'on': 1559, 'that': 1550, 's': 1404, 'as': 1100, 'said': 1096, 'with': 1071, 'climate': 1014, 'are': 975, 'it': 942, 'by': 891, 'at': 863, 'be': 807, 'from': 763, 'have': 670, 'we': 659, 'this': 644, 'will': 638}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T03:19:53.968485Z",
     "start_time": "2025-03-01T03:19:53.932849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "weighted_dict= {}\n",
    "\n",
    "# 1) Original weighted dictionary\n",
    "# weighted_dict = {\n",
    "#     \"arts\": {\n",
    "#         \"artist\": 1.0,\n",
    "#         \"exhibition\": 0.9,\n",
    "#         \"gallery\": 0.9,\n",
    "#         \"sculpture\": 0.8,\n",
    "#         \"performance\": 0.8,\n",
    "#         \"museum\": 0.7,\n",
    "#         \"creative\": 0.6,\n",
    "#         \"painting\": 0.7,\n",
    "#     },\n",
    "#     \"crime\": {\n",
    "#         \"crime\": 1.0,\n",
    "#         \"theft\": 0.9,\n",
    "#         \"murder\": 0.95,\n",
    "#         \"police\": 0.8,\n",
    "#         \"arrest\": 0.85,\n",
    "#         \"fraud\": 0.75,\n",
    "#         \"investigation\": 0.7,\n",
    "#         \"law\": 0.6,\n",
    "#     },\n",
    "#     \"disaster\": {\n",
    "#         \"earthquake\": 0.95,\n",
    "#         \"flood\": 0.9,\n",
    "#         \"hurricane\": 0.9,\n",
    "#         \"disaster\": 1.0,\n",
    "#         \"emergency\": 0.8,\n",
    "#         \"rescue\": 0.75,\n",
    "#         \"casualty\": 0.7,\n",
    "#         \"evacuation\": 0.7,\n",
    "#         \"fire\": 0.9,\n",
    "#     },\n",
    "#     \"economy\": {\n",
    "#         \"gdp\": 0.9,\n",
    "#         \"inflation\": 0.95,\n",
    "#         \"market\": 0.85,\n",
    "#         \"trade\": 0.8,\n",
    "#         \"stock\": 0.75,\n",
    "#         \"recession\": 0.9,\n",
    "#         \"currency\": 0.7,\n",
    "#         \"deficit\": 0.7,\n",
    "#     },\n",
    "#     \"education\": {\n",
    "#         \"school\": 0.9,\n",
    "#         \"student\": 0.95,\n",
    "#         \"teacher\": 0.85,\n",
    "#         \"university\": 0.8,\n",
    "#         \"curriculum\": 0.7,\n",
    "#         \"tuition\": 0.7,\n",
    "#         \"degree\": 0.6,\n",
    "#         \"exam\": 0.6,\n",
    "#     },\n",
    "#     \"environmental\": {\n",
    "#         \"pollution\": 0.95,\n",
    "#         \"climate\": 0.9,\n",
    "#         \"recycling\": 0.8,\n",
    "#         \"sustainability\": 0.85,\n",
    "#         \"carbon\": 0.9,\n",
    "#         \"emissions\": 0.85,\n",
    "#         \"wildlife\": 0.7,\n",
    "#         \"conservation\": 0.75,\n",
    "#     },\n",
    "#     \"health\": {\n",
    "#         \"hospital\": 0.9,\n",
    "#         \"disease\": 0.95,\n",
    "#         \"vaccine\": 0.85,\n",
    "#         \"patient\": 0.8,\n",
    "#         \"medical\": 0.85,\n",
    "#         \"pandemic\": 1.0,\n",
    "#         \"treatment\": 0.7,\n",
    "#         \"mental\": 0.6,\n",
    "#     },\n",
    "#     \"humaninterest\": {\n",
    "#         \"story\": 0.9,\n",
    "#         \"award\": 1.0,\n",
    "#         \"community\": 0.8,\n",
    "#         \"charity\": 0.75,\n",
    "#         \"volunteer\": 0.7,\n",
    "#         \"personal\": 0.85,\n",
    "#         \"inspiration\": 0.7,\n",
    "#         \"family\": 0.6,\n",
    "#         \"achievement\": 0.65,\n",
    "#     },\n",
    "#     \"labour\": {\n",
    "#         \"union\": 0.9,\n",
    "#         \"employment\": 0.8,\n",
    "#         \"strike\": 0.85,\n",
    "#         \"wage\": 0.9,\n",
    "#         \"contract\": 0.7,\n",
    "#         \"workers\": 0.95,\n",
    "#         \"work\": 0.9,\n",
    "#         \"job\": 0.85,\n",
    "#         \"security\": 0.85,\n",
    "#         \"benefits\": 0.6,\n",
    "#         \"overtime\": 0.6,\n",
    "#     },\n",
    "#     \"lifestyle\": {\n",
    "#         \"fashion\": 0.9,\n",
    "#         \"travel\": 0.85,\n",
    "#         \"diet\": 0.8,\n",
    "#         \"hobby\": 0.7,\n",
    "#         \"wellness\": 0.75,\n",
    "#         \"luxury\": 0.6,\n",
    "#         \"trend\": 0.7,\n",
    "#         \"fitness\": 0.6,\n",
    "#         \"home\": 0.85,\n",
    "#     },\n",
    "#     \"politics\": {\n",
    "#         \"government\": 0.95,\n",
    "#         \"election\": 0.9,\n",
    "#         \"policy\": 0.85,\n",
    "#         \"diplomacy\": 0.8,\n",
    "#         \"senate\": 0.75,\n",
    "#         \"vote\": 0.9,\n",
    "#         \"democracy\": 0.7,\n",
    "#         \"law\": 0.6,\n",
    "#         \"president\": 0.9,\n",
    "#         \"state\": 0.8,\n",
    "#     },\n",
    "#     \"religion\": {\n",
    "#         \"faith\": 0.9,\n",
    "#         \"temple\": 0.8,\n",
    "#         \"prayer\": 0.85,\n",
    "#         \"ritual\": 0.75,\n",
    "#         \"church\": 0.8,\n",
    "#         \"islam\": 0.7,\n",
    "#         \"christianity\": 0.7,\n",
    "#         \"worship\": 0.7,\n",
    "#         \"god\": 0.85,\n",
    "#         \"spiritual\": 0.8,\n",
    "#     },\n",
    "#     \"science\": {\n",
    "#         \"research\": 0.95,\n",
    "#         \"experiment\": 0.9,\n",
    "#         \"technology\": 0.85,\n",
    "#         \"data\": 0.8,\n",
    "#         \"discovery\": 0.85,\n",
    "#         \"physics\": 0.75,\n",
    "#         \"biology\": 0.7,\n",
    "#         \"innovation\": 0.7,\n",
    "#     },\n",
    "#     \"social\": {\n",
    "#         \"equality\": 0.9,\n",
    "#         \"welfare\": 0.85,\n",
    "#         \"activism\": 0.8,\n",
    "#         \"diversity\": 0.75,\n",
    "#         \"rights\": 0.9,\n",
    "#         \"justice\": 0.7,\n",
    "#         \"community\": 0.6,\n",
    "#         \"protest\": 0.65,\n",
    "#         \"society\": 0.8,\n",
    "#     },\n",
    "#     \"sport\": {\n",
    "#         \"athlete\": 0.95,\n",
    "#         \"match\": 0.9,\n",
    "#         \"tournament\": 0.85,\n",
    "#         \"score\": 0.8,\n",
    "#         \"league\": 0.75,\n",
    "#         \"olympics\": 0.7,\n",
    "#         \"training\": 0.6,\n",
    "#         \"championship\": 0.8,\n",
    "#     },\n",
    "#     \"unrest\": {\n",
    "#         \"protest\": 0.9,\n",
    "#         \"riot\": 0.95,\n",
    "#         \"clash\": 0.85,\n",
    "#         \"violence\": 0.9,\n",
    "#         \"tension\": 0.8,\n",
    "#         \"conflict\": 0.75,\n",
    "#         \"demonstration\": 0.7,\n",
    "#         \"angry\": 0.6,\n",
    "#     },\n",
    "#     \"weather\": {\n",
    "#         \"forecast\": 0.9,\n",
    "#         \"storm\": 0.95,\n",
    "#         \"temperature\": 0.8,\n",
    "#         \"rain\": 0.75,\n",
    "#         \"drought\": 0.7,\n",
    "#         \"sunny\": 0.6,\n",
    "#         \"climate\": 0.6,\n",
    "#         \"hurricane\": 0.7,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# 2) New words from your dataset (per category)\n",
    "new_keywords = {\n",
    "    'arts': [\n",
    "        'music','show','series','park','best','million','event','back','work','state',\n",
    "        'way','may','get','group','take','world','community','season','good','school'\n",
    "    ],\n",
    "    'crime': [\n",
    "        'police','court','law','state','case','county','old','found','may','hearing',\n",
    "        'death','federal','prison','told','justice','man','right','officer','according','president'\n",
    "    ],\n",
    "    'disaster': [\n",
    "        'fire','water','police','air','state','rescue','city','may','area','county',\n",
    "        'home','near','according','news','around','could','back','national','help','road'\n",
    "    ],\n",
    "    'economy': [\n",
    "        'company','may','per','market','million','price','tax','billion','business','government',\n",
    "        'state','since','industry','group','percent','use','get','made','work','according'\n",
    "    ],\n",
    "    'education': [\n",
    "        'school','university','education','high','learning','college','state','student','class',\n",
    "        'community','work','program','support','help','board','government','president','covid','public','national'\n",
    "    ],\n",
    "    'environmental': [\n",
    "        'energy','park','waste','water','power','market','state','company','north','management',\n",
    "        'solar','group','renewable','county','world','wildlife','city','wind','river','industry'\n",
    "    ],\n",
    "    'health': [\n",
    "        'health','covid','care','market','medical','pandemic','hospital','state','may','vaccine',\n",
    "        'public','food','disease','get','help','company','global','well','virus','work'\n",
    "    ],\n",
    "    'humaninterest': [\n",
    "        'award','size','get','act','home','back','winning','best','statement','texas',\n",
    "        'team','may','good','world','well','ownership','beneficial','plant','second','top'\n",
    "    ],\n",
    "    'labour': [\n",
    "        'work','health','security','working','pandemic','retirement','may','covid','could','get',\n",
    "        'state','make','job','business','home','help','social','market','city','week'\n",
    "    ],\n",
    "    'lifestyle': [\n",
    "        'home','travel','get','beauty','bridge','make','summer','may','way','life',\n",
    "        'work','back','million','come','food','covid','want','pandemic','best','business'\n",
    "    ],\n",
    "    'other': [\n",
    "        'state','market','game','covid','government','may','could','rub','wednesday','home',\n",
    "        'high','city','get','school','pandemic','health','make','season','back','team'\n",
    "    ],\n",
    "    'politics': [\n",
    "        'government','state','president','minister','data','security','public','city','international','support',\n",
    "        'tuesday','news','united','may','covid','national','pandemic','help','house','health'\n",
    "    ],\n",
    "    'religion': [\n",
    "        'church','life','school','family','may','community','world','way','come','work',\n",
    "        'home','covid','old','get','back','spiritual','god','state','well','city'\n",
    "    ],\n",
    "    'science': [\n",
    "        'market','research','space','report','global','data','work','company','study','industry',\n",
    "        'may','science','medical','could','high','design','engineering','covid','well','use'\n",
    "    ],\n",
    "    'social': [\n",
    "        'health','care','social','work','government','covid','may','baby','state','abortion',\n",
    "        'support','home','get','life','pandemic','according','community','make','city','help'\n",
    "    ],\n",
    "    'sport': [\n",
    "        'team','game','season','back','world','get','match','second','sport','four',\n",
    "        'league','win','best','final','play','tournament','coach','right','open','championship'\n",
    "    ],\n",
    "    'unrest': [\n",
    "        'israel','police','war','government','city','violence','shooting','israeli','military','may',\n",
    "        'wednesday','state','gun','country','president','old','come','security','world','home'\n",
    "    ],\n",
    "    'weather': [\n",
    "        'climate','change','heat','weather','water','high','carbon','could','global','national',\n",
    "        'state','may','risk','area','service','according','world','friday','rain','report'\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 3) Create a copy so we don't modify original directly\n",
    "updated_dict = copy.deepcopy(weighted_dict)\n",
    "\n",
    "# 4) Define a default weight for new words\n",
    "DEFAULT_WEIGHT = 0.6\n",
    "\n",
    "# 5) Merge logic\n",
    "for category, words in new_keywords.items():\n",
    "    # If this category doesn't exist in the dictionary, create it\n",
    "    if category not in updated_dict:\n",
    "        updated_dict[category] = {}\n",
    "    \n",
    "    for w in words:\n",
    "        # Add the word only if it isn't already in the dictionary\n",
    "        if w not in updated_dict[category]:\n",
    "            updated_dict[category][w] = DEFAULT_WEIGHT\n",
    "\n",
    "# 'updated_dict' is now the merged result.\n",
    "\n",
    "\n",
    "# x = df[\"clean_text\"]\n",
    "# y = df[\"category\"]\n",
    "# # Train the Random Forest classifier\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     x, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "# \n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "# X_test_vectorized = vectorizer.transform(X_test)\n",
    "# \n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train_vectorized, y_train)\n",
    "# \n",
    "# y_pred = rf.predict(X_test_vectorized)\n",
    "# \n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# \n",
    "# def dictionary_score(sentence, weighted_dict):\n",
    "#     \"\"\"\n",
    "#     Computes a score for each class based on the presence of weighted keywords.\n",
    "#     \"\"\"\n",
    "#     words = sentence.lower().split()\n",
    "#     scores = {cls: 0 for cls in weighted_dict}\n",
    "#     for cls in weighted_dict:\n",
    "#         for word in words:\n",
    "#             scores[cls] += weighted_dict[cls].get(word, 0)\n",
    "#     return scores\n",
    "# \n",
    "# def classifier_predict(sentence):\n",
    "#     X_sentence = vectorizer.transform([sentence])\n",
    "#     probas = rf.predict_proba(X_sentence)[0]\n",
    "#     return dict(zip(rf.classes_, probas))\n",
    "# \n",
    "# def combine_predictions(clf_pred, dict_scores, alpha=0.5):\n",
    "#     \"\"\"\n",
    "#     Combine classifier probabilities and dictionary scores.\n",
    "#     The parameter alpha weights the classifier output.\n",
    "#     \"\"\"\n",
    "#     combined = {}\n",
    "#     for cls in clf_pred:\n",
    "#         combined[cls] = alpha * clf_pred[cls] + (1 - alpha) * dict_scores.get(cls, 0)\n",
    "#     return combined\n",
    "# \n",
    "# def classify_text(text, alpha=0.5):\n",
    "#     \"\"\"\n",
    "#     Tokenize the text into sentences, combine classifier and dictionary scores,\n",
    "#     and return the final predicted class based on majority vote.\n",
    "#     \"\"\"\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     sentence_labels = []\n",
    "#     \n",
    "#     for sentence in sentences:\n",
    "#         clf_pred = classifier_predict(sentence)\n",
    "#         dict_scores = dictionary_score(sentence, weighted_dict)\n",
    "#         combined_scores = combine_predictions(clf_pred, dict_scores, alpha)\n",
    "#         # Select the class with the highest combined score for the sentence\n",
    "#         sentence_label = max(combined_scores, key=combined_scores.get)\n",
    "#         sentence_labels.append(sentence_label)\n",
    "#     \n",
    "#     # Final classification by taking the most common class (majority vote)\n",
    "#     final_label = Counter(sentence_labels).most_common(1)[0][0]\n",
    "#     return final_label\n",
    "# \n",
    "# # Example usage:\n",
    "# text_row = (\"The gallery showcased a variety of paintings and sculptures. \"\n",
    "#             \"A recent experiment in the laboratory produced groundbreaking results.\")\n",
    "# final_prediction = classify_text(text_row, alpha=0.6)\n",
    "# print(\"Final predicted class:\", final_prediction)"
   ],
   "id": "fc247fb1c619db24",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T03:23:31.765919Z",
     "start_time": "2025-03-01T03:19:58.867787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dictionary_score(sentence, weighted_dict):\n",
    "    \"\"\"\n",
    "    Computes a score for each class based on the presence of weighted keywords.\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    scores = {cls: 0 for cls in weighted_dict}\n",
    "    for cls in weighted_dict:\n",
    "        for word in words:\n",
    "            scores[cls] += weighted_dict[cls].get(word, 0)\n",
    "    return scores\n",
    "\n",
    "def classifier_predict(sentence):\n",
    "    \"\"\"\n",
    "    Uses the trained Random Forest and TF-IDF vectorizer to get class probabilities.\n",
    "    \"\"\"\n",
    "    X_sentence = vectorizer.transform([sentence])\n",
    "    probas = rf.predict_proba(X_sentence)[0]\n",
    "    return dict(zip(rf.classes_, probas))\n",
    "\n",
    "def combine_predictions(clf_pred, dict_scores, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Combine classifier probabilities and dictionary scores.\n",
    "    The parameter alpha weights the classifier output.\n",
    "    \"\"\"\n",
    "    combined = {}\n",
    "    for cls in clf_pred:\n",
    "        combined[cls] = alpha * clf_pred[cls] + (1 - alpha) * dict_scores.get(cls, 0)\n",
    "    return combined\n",
    "\n",
    "def classify_text(text, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Tokenize the text into sentences, combine classifier and dictionary scores,\n",
    "    and return the final predicted class based on majority vote.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_labels = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        clf_pred = classifier_predict(sentence)\n",
    "        dict_scores = dictionary_score(sentence, weighted_dict)\n",
    "        combined_scores = combine_predictions(clf_pred, dict_scores, alpha)\n",
    "        \n",
    "        # Select the class with the highest combined score for the sentence\n",
    "        sentence_label = max(combined_scores, key=combined_scores.get)\n",
    "        sentence_labels.append(sentence_label)\n",
    "    \n",
    "    # Final classification by taking the most common class (majority vote)\n",
    "    final_label = Counter(sentence_labels).most_common(1)[0][0]\n",
    "    return final_label\n",
    "\n",
    "# \n",
    "# df_bad = pd.read_csv(\"ethanCleaned.csv\")\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Enable progress_apply on pandas objects\n",
    "import numpy as np\n",
    "\n",
    "df[\"predicted_category\"] = df[\"clean_text\"].progress_apply(lambda x: classify_text(x, alpha=0.5))\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# df_bad[\"predicted_category\"] = np.where(\n",
    "#     (df_bad[\"matches\"] != 1.0) | df_bad[\"matches\"].isna(),\n",
    "#     df_bad[\"clean_text\"].astype(str).progress_apply(lambda x: classify_text(x, alpha=0.5) if pd.notna(x) and isinstance(x, str) else np.nan),\n",
    "#     np.nan  # Assign NaN where matches == 1.0\n",
    "# )\n",
    "# \n",
    "# df_bad.head()\n"
   ],
   "id": "ceef9dd956dd293f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5292/5292 [03:32<00:00, 24.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  category                                               text  \\\n",
       "0     arts  rob delaney vir das galen hopper samson kayo g...   \n",
       "1     arts  andris nelsons conducts a joint concert of the...   \n",
       "2     arts  warner music group has brought on sherry tan t...   \n",
       "3     arts  adele will explore what she s been going throu...   \n",
       "4     arts  you are using an older browser version please ...   \n",
       "\n",
       "                                          clean_text predicted_category  \n",
       "0  rob da galen hopper samson kayo guz khan nick ...             health  \n",
       "1  nelson conduct joint concert boston symphony o...             labour  \n",
       "2  warner music group brought sherry tan head mus...           politics  \n",
       "3  explore going new album set explore going new ...      humaninterest  \n",
       "4  older browser version please use version best ...           politics  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts</td>\n",
       "      <td>rob delaney vir das galen hopper samson kayo g...</td>\n",
       "      <td>rob da galen hopper samson kayo guz khan nick ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts</td>\n",
       "      <td>andris nelsons conducts a joint concert of the...</td>\n",
       "      <td>nelson conduct joint concert boston symphony o...</td>\n",
       "      <td>labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts</td>\n",
       "      <td>warner music group has brought on sherry tan t...</td>\n",
       "      <td>warner music group brought sherry tan head mus...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts</td>\n",
       "      <td>adele will explore what she s been going throu...</td>\n",
       "      <td>explore going new album set explore going new ...</td>\n",
       "      <td>humaninterest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts</td>\n",
       "      <td>you are using an older browser version please ...</td>\n",
       "      <td>older browser version please use version best ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T22:55:16.805332Z",
     "start_time": "2025-02-27T22:55:16.796920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Update 'category' only where 'predicted_category' is not NaN\n",
    "# df_bad[\"category\"] = np.where(df_bad[\"predicted_category\"].notna(), df_bad[\"predicted_category\"], df_bad[\"category\"])\n",
    "# \n",
    "# # Create 'total_category' to reflect the final combined categories\n",
    "# df[\"total_category\"] = df_bad[\"category\"]\n",
    "# df.head()"
   ],
   "id": "95c5226b2f7b5801",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  category                                               text  \\\n",
       "0     arts  you are using an older browser version. please...   \n",
       "1     arts  on 31 march two of classical music s most acco...   \n",
       "2     arts  bpt after a year of being locked away at home ...   \n",
       "3     arts  pilot uninjured plane hit sandbar while landin...   \n",
       "4     arts  colleen distin photo by facebook toronto sun ....   \n",
       "\n",
       "                                          clean_text  words_removed  \\\n",
       "0  you are an older browser version . please use ...             50   \n",
       "1  on 31 march two of classical music s most acco...            156   \n",
       "2  after a year of being locked away at home the ...             98   \n",
       "3  pilot uninjured plane hit while landing . a fl...             22   \n",
       "4  colleen photo by sun . a lost wallet been retu...             47   \n",
       "\n",
       "                                               text2 total_category  \n",
       "0  older browser version please use version best ...           arts  \n",
       "1  march two classical music accomplished well kn...           arts  \n",
       "2  year locked away home world eager reopen exper...           arts  \n",
       "3  pilot uninjured plane hit landing float plane ...  humaninterest  \n",
       "4  colleen photo sun lost wallet returned owner c...           arts  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>words_removed</th>\n",
       "      <th>text2</th>\n",
       "      <th>total_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts</td>\n",
       "      <td>you are using an older browser version. please...</td>\n",
       "      <td>you are an older browser version . please use ...</td>\n",
       "      <td>50</td>\n",
       "      <td>older browser version please use version best ...</td>\n",
       "      <td>arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts</td>\n",
       "      <td>on 31 march two of classical music s most acco...</td>\n",
       "      <td>on 31 march two of classical music s most acco...</td>\n",
       "      <td>156</td>\n",
       "      <td>march two classical music accomplished well kn...</td>\n",
       "      <td>arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts</td>\n",
       "      <td>bpt after a year of being locked away at home ...</td>\n",
       "      <td>after a year of being locked away at home the ...</td>\n",
       "      <td>98</td>\n",
       "      <td>year locked away home world eager reopen exper...</td>\n",
       "      <td>arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts</td>\n",
       "      <td>pilot uninjured plane hit sandbar while landin...</td>\n",
       "      <td>pilot uninjured plane hit while landing . a fl...</td>\n",
       "      <td>22</td>\n",
       "      <td>pilot uninjured plane hit landing float plane ...</td>\n",
       "      <td>humaninterest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts</td>\n",
       "      <td>colleen distin photo by facebook toronto sun ....</td>\n",
       "      <td>colleen photo by sun . a lost wallet been retu...</td>\n",
       "      <td>47</td>\n",
       "      <td>colleen photo sun lost wallet returned owner c...</td>\n",
       "      <td>arts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T03:24:15.537811Z",
     "start_time": "2025-03-01T03:24:08.085145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords, words\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "import re\n",
    "# Download NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_vocab = set(w.lower() for w in words.words())\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text, removing\n",
    "    URLs, HTML tags, punctuation, stopwords, and any word\n",
    "    that is not in a large English vocabulary set.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Step 2: Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Step 3: Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Step 4: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 5: Tokenize text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Step 6: Remove stopwords and short tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) >= 3]\n",
    "    \n",
    "    # Step 7: Lemmatize words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Step 8: Keep only English words (based on nltk.corpus.words)\n",
    "    tokens = [word for word in tokens if word in english_vocab]\n",
    "\n",
    "    # Step 9: Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "df.head()"
   ],
   "id": "187ce0674ef91a4b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/karennurlybekov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/karennurlybekov/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/karennurlybekov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/karennurlybekov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  category                                               text  \\\n",
       "0     arts  rob delaney vir das galen hopper samson kayo g...   \n",
       "1     arts  andris nelsons conducts a joint concert of the...   \n",
       "2     arts  warner music group has brought on sherry tan t...   \n",
       "3     arts  adele will explore what she s been going throu...   \n",
       "4     arts  you are using an older browser version please ...   \n",
       "\n",
       "                                          clean_text predicted_category  \n",
       "0  rob da galen hopper samson kayo guz khan nick ...             health  \n",
       "1  nelson conduct joint concert boston symphony o...             labour  \n",
       "2  warner music group brought sherry tan head mus...           politics  \n",
       "3  explore going new album set explore going new ...      humaninterest  \n",
       "4  older browser version please use version best ...           politics  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts</td>\n",
       "      <td>rob delaney vir das galen hopper samson kayo g...</td>\n",
       "      <td>rob da galen hopper samson kayo guz khan nick ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts</td>\n",
       "      <td>andris nelsons conducts a joint concert of the...</td>\n",
       "      <td>nelson conduct joint concert boston symphony o...</td>\n",
       "      <td>labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts</td>\n",
       "      <td>warner music group has brought on sherry tan t...</td>\n",
       "      <td>warner music group brought sherry tan head mus...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts</td>\n",
       "      <td>adele will explore what she s been going throu...</td>\n",
       "      <td>explore going new album set explore going new ...</td>\n",
       "      <td>humaninterest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts</td>\n",
       "      <td>you are using an older browser version please ...</td>\n",
       "      <td>older browser version please use version best ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T03:24:24.893536Z",
     "start_time": "2025-03-01T03:24:24.890325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# df[\"total_category\"] = np.where(df[\"category\"] == \"other\", \"other\", df[\"predicted_category\"])\n",
    "\n",
    "df['predicted_category'].value_counts()\n",
    "\n",
    "    "
   ],
   "id": "43762f675050952e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_category\n",
       "politics         2631\n",
       "education         482\n",
       "health            380\n",
       "sport             324\n",
       "crime             295\n",
       "humaninterest     229\n",
       "environmental     162\n",
       "labour            158\n",
       "economy           138\n",
       "weather           127\n",
       "lifestyle         117\n",
       "science           110\n",
       "disaster           73\n",
       "religion           45\n",
       "arts               15\n",
       "unrest              4\n",
       "social              2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T03:00:53.856353Z",
     "start_time": "2025-03-01T02:59:20.462506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Example DataFrame\n",
    "# df has columns \"text2\" and \"total_category\"\n",
    "x = df[\"text\"]\n",
    "y = df[\"predicted_category\"]\n",
    "\n",
    "# 1. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_depth\": [5, 10, 15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    # 'max_features': ['sqrt', 'log2', None],  # Optionally include this\n",
    "    \"bootstrap\": [True, False], \n",
    "    \"criterion\": [\"gini\", \"entropy\"],  \n",
    "    # You can also consider tuning class_weight, e.g.:\n",
    "    # \"class_weight\": [\"balanced\", \"balanced_subsample\", None]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 3. Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300, \n",
    "    random_state=42, \n",
    "    class_weight=\"balanced\", \n",
    ")\n",
    "rf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# 4. Get the usual predictions\n",
    "y_pred = rf.predict(X_test_vectorized)\n",
    "\n",
    "# 5. Get predicted probabilities\n",
    "y_proba = rf.predict_proba(X_test_vectorized)\n",
    "\n",
    "# 6. Define a threshold for \"not sure\"\n",
    "threshold = 0.2\n",
    "\n",
    "# 7. Create a new set of predictions, switching to \"other\" if max probability < threshold\n",
    "y_pred_with_other = []\n",
    "for i, probs in enumerate(y_proba):\n",
    "    if np.max(probs) < threshold:\n",
    "        y_pred_with_other.append(\"other\")\n",
    "    else:\n",
    "        # Keep the original predicted class\n",
    "        y_pred_with_other.append(y_pred[i])\n",
    "\n",
    "# 8. Evaluate\n",
    "# Note: If you include \"other\" in predictions but it is not in y_test, \n",
    "#       classification_report will show 0 for precision/recall/f1 of \"other\"\n",
    "#       and metrics for original classes will not change.\n",
    "accuracy_with_other = accuracy_score(y_test, y_pred_with_other)\n",
    "print(\"Accuracy (with 'other'):\", accuracy_with_other)\n",
    "\n",
    "print(\"\\nClassification Report (with 'other'):\")\n",
    "print(classification_report(y_test, y_pred_with_other))"
   ],
   "id": "4c47c0b6e8d1eb9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with 'other'): 0.5051935788479698\n",
      "\n",
      "Classification Report (with 'other'):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         arts       1.00      0.04      0.08        23\n",
      "        crime       0.68      0.74      0.71        65\n",
      "     disaster       0.75      0.12      0.21        48\n",
      "      economy       0.81      0.32      0.46        41\n",
      "    education       0.81      0.79      0.80        97\n",
      "environmental       0.77      0.54      0.64        50\n",
      "       health       0.71      0.63      0.67       103\n",
      "humaninterest       0.67      0.36      0.47       106\n",
      "       labour       0.77      0.29      0.42        94\n",
      "    lifestyle       0.88      0.11      0.20        62\n",
      "        other       0.00      0.00      0.00         0\n",
      "     politics       0.44      0.83      0.58       172\n",
      "     religion       1.00      0.28      0.43        29\n",
      "      science       0.79      0.24      0.37        46\n",
      "       social       0.00      0.00      0.00        14\n",
      "        sport       0.61      0.72      0.66        58\n",
      "       unrest       1.00      0.06      0.11        17\n",
      "      weather       0.85      0.65      0.73        34\n",
      "\n",
      "     accuracy                           0.51      1059\n",
      "    macro avg       0.70      0.37      0.42      1059\n",
      " weighted avg       0.71      0.51      0.52      1059\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T04:29:28.578589Z",
     "start_time": "2025-03-01T04:28:24.932171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example DataFrame: df with columns \"text2\" and \"total_category\"\n",
    "x = df[\"text\"]\n",
    "y = df[\"predicted_category\"]\n",
    "\n",
    "# 1. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    max_features=50000,\n",
    "    sublinear_tf=True,\n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# 3. Define parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "    \"max_depth\": [5, 10, 15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"class_weight\": [\"balanced\", None, \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "# 4. Base RandomForest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 5. RandomizedSearchCV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 6. Fit random search on training data\n",
    "random_search.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# 7. Get the best model and hyperparameters\n",
    "best_rf = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters found by RandomizedSearchCV:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# 8. Predict on test data (standard prediction, no custom threshold)\n",
    "y_pred = best_rf.predict(X_test_vectorized)\n",
    "\n",
    "# 9. Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "26fbdbc4c2153511",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters found by RandomizedSearchCV:\n",
      "{'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': None, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'bootstrap': True}\n",
      "\n",
      "Accuracy: 0.8083097261567517\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         arts       1.00      1.00      1.00         1\n",
      "        crime       0.75      0.92      0.83        63\n",
      "     disaster       0.57      0.67      0.62         6\n",
      "      economy       0.84      0.80      0.82        20\n",
      "    education       0.80      0.92      0.86        93\n",
      "environmental       0.64      0.90      0.75        31\n",
      "       health       0.69      0.89      0.78        66\n",
      "humaninterest       0.47      0.23      0.31        30\n",
      "       labour       0.59      0.54      0.57        24\n",
      "    lifestyle       0.50      0.33      0.40        15\n",
      "     politics       0.89      0.80      0.84       598\n",
      "     religion       0.60      0.60      0.60         5\n",
      "      science       0.81      0.72      0.76        18\n",
      "        sport       0.74      0.97      0.84        67\n",
      "      weather       0.80      0.91      0.85        22\n",
      "\n",
      "     accuracy                           0.81      1059\n",
      "    macro avg       0.71      0.75      0.72      1059\n",
      " weighted avg       0.81      0.81      0.80      1059\n",
      "\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/166 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12ff0f2043a147faa9a73c0626d7d1f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "corpus = df[\"clean_text\"].tolist()\n",
    "embeddings = embedder.encode(corpus, show_progress_bar=True)"
   ],
   "id": "bbaaa7863850dcbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T00:06:31.843849Z",
     "start_time": "2025-02-28T00:06:31.798722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = embeddings\n",
    "y = df[\"predicted_category\"].values\n",
    "\n",
    "# Split data into train/test (or train/val/test if you prefer)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [None, 5, 10],\n",
    "    \"max_features\": ['sqrt', 'log2'],\n",
    "    \"class_weight\": ['balanced_subsample', 'balanced', None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    # Add more params if desired, e.g. \"min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf_clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=2,               # 3-fold cross validation\n",
    "    scoring=\"accuracy\", # or \"accuracy\", \"f1_weighted\", etc.\n",
    "    n_jobs=-1,          # utilize all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV score (accuracy):\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nTEST SET PERFORMANCE:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ],
   "id": "f2a8a35d1149f021",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_category'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'predicted_category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Suppose df has a column \"predicted_category\" for labels\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# and you have embeddings as your features (NumPy array or similar)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m X \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext2\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 11\u001B[0m y \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredicted_category\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# 1. Train-Test Split\u001B[39;00m\n\u001B[1;32m     14\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[1;32m     15\u001B[0m     X,\n\u001B[1;32m     16\u001B[0m     y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m     stratify\u001B[38;5;241m=\u001B[39my\n\u001B[1;32m     20\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'predicted_category'"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T00:32:13.549313Z",
     "start_time": "2025-02-28T00:18:26.642854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Data\n",
    "df_bad = pd.read_csv(\"ethanCleaned.csv\")\n",
    "\n",
    "# Drop rows where \"matches\" or \"clean_text\" or \"category\" is NaN\n",
    "df_bad = df_bad.dropna(subset=[\"matches\", \"clean_text\", \"category\"])\n",
    "\n",
    "# Extract features (X) and target labels (y)\n",
    "X = df_bad[\"clean_text\"]\n",
    "y = df_bad[\"category\"]\n",
    "\n",
    "# 1. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_vectorized, y_train = ros.fit_resample(X_train_vectorized, y_train)\n",
    "\n",
    "# 3. Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300, \n",
    "    random_state=42, \n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "rf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# 4. Get the usual predictions\n",
    "y_pred = rf.predict(X_test_vectorized)\n",
    "\n",
    "# 5. Get predicted probabilities\n",
    "y_proba = rf.predict_proba(X_test_vectorized)\n",
    "\n",
    "\n",
    "# 8. Evaluate\n",
    "accuracy_with_other = accuracy_score(y_test, y_pred_with_other)\n",
    "print(\"Accuracy (with 'other'):\", accuracy_with_other)\n",
    "\n",
    "print(\"\\nClassification Report (with 'other'):\")\n",
    "print(classification_report(y_test, y_pred_with_other))\n"
   ],
   "id": "db8115d3d079feea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with 'other'): 0.21676646706586827\n",
      "\n",
      "Classification Report (with 'other'):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         arts       1.00      0.05      0.10        56\n",
      "        crime       0.79      0.29      0.42        52\n",
      "     disaster       0.82      0.20      0.32        45\n",
      "      economy       1.00      0.07      0.12        46\n",
      "    education       0.68      0.50      0.58        52\n",
      "environmental       1.00      0.21      0.34        58\n",
      "       health       0.53      0.18      0.27        45\n",
      "humaninterest       1.00      0.02      0.05        43\n",
      "       labour       0.67      0.05      0.09        43\n",
      "    lifestyle       0.00      0.00      0.00        50\n",
      "        other       0.05      0.72      0.08        39\n",
      "     politics       1.00      0.02      0.04        49\n",
      "     religion       0.75      0.18      0.29        33\n",
      "      science       0.42      0.11      0.17        47\n",
      "       social       0.00      0.00      0.00        43\n",
      "        sport       0.50      0.50      0.50        42\n",
      "       unrest       0.86      0.15      0.26        40\n",
      "      weather       0.92      0.67      0.78        52\n",
      "\n",
      "     accuracy                           0.22       835\n",
      "    macro avg       0.67      0.22      0.24       835\n",
      " weighted avg       0.68      0.22      0.25       835\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/karennurlybekov/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e90499d65806982"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
